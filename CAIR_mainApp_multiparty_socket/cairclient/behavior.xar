<?xml version="1.0" encoding="UTF-8" ?><ChoregrapheProject xmlns="http://www.aldebaran-robotics.com/schema/choregraphe/project.xsd" xar_version="3"><Box name="root" id="-1" localization="8" tooltip="Root box of Choregraphe&apos;s behavior. Highest level possible." x="0" y="0"><bitmap>media/images/box/root.png</bitmap><script language="4"><content><![CDATA[]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /><Timeline enable="0"><BehaviorLayer name="behavior_layer1"><BehaviorKeyframe name="keyframe1" index="1"><Diagram><Box name="CAIR client" id="1" localization="8" tooltip="This box contains a basic python script and can be used to create any python script box you would like.&#x0A;&#x0A;To edit its script, double-click on it." x="333" y="156"><bitmap>media/images/box/box-python-script.png</bitmap><script language="4"><content><![CDATA[import qi
import sys
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/cairapp/libs/")
sys.path.append("/data/home/nao/.local/share/PackageManager/apps/cairapp/")
from naoqi import ALProxy
import threading
import requests
import xml.etree.ElementTree as ET
import time
import json
import os
import manage_actions
import client_utils
import socket
import zlib

audio_recorder_host = "130.251.13.147"
audio_recorder_port = 9090

server_IP = "130.251.13.147"
BASE = "http://" + server_IP + ":5000/CAIR_hub"


class MyClass(GeneratedClass):
    def __init__(self):
        GeneratedClass.__init__(self)
        # Instances of the classes of the other files in the libs folder containing functions needed here
        self.plans = manage_actions.ActionManager(self.logger.info)
        self.utils = client_utils.Utils(self.logger.info)
        self.utils.setAutonomousAbilities(True, True, True, True, True)
        self.isAlive = True
        self.exit_keywords = ["stop talking", "quit the application"]
        self.repeat_keywords = ["repeat", "say it again"]
        self.memory = ALProxy("ALMemory")
        self.sASR = ALProxy("ASR2")
        self.speech_reco_event = "Audio/RecognizedWords"
        self.al = ALProxy("ALAutonomousLife")
        self.motion = ALProxy("ALMotion")
        self.tts = ALProxy("ALTextToSpeech")
        self.animated_speech = ALProxy("ALAnimatedSpeech")
        self.configuration = {"bodyLanguageMode": "contextual"}
        self.behavior_manager = ALProxy("ALBehaviorManager")
        self.audio_device = ALProxy("ALAudioDevice")
        self.speakers_info_file_path = "/data/home/nao/.local/share/PackageManager/apps/cairapp/speakers_info.json"
        self.speakers_stats_file_path = "/data/home/nao/.local/share/PackageManager/apps/cairapp/speakers_stats.json"
        self.not_installed_behavior = "I'm sorry, I can't perform this task because the behavior is not installed."
        # To store the previous sentence said by the robot
        self.previous_sentence = ""
        # This variable tells if the user want the robot to repeat a sentence
        self.repeat = False

        # Store the number of people in front of the robot, coming from the sensor readings
        self.people = 0

        self.logger.info("Trying to connect to ALTabletService...")
        self.tablet = True
        try:
            self.tablet_service = ALProxy("ALTabletService")
        except:
            self.tablet = False

        self.logger.info("Setting voice speed")
        try:
            # self.voice_speed = "\\RSPD=100\\"
            self.voice_speed = "\\RSPD=" + str(self.memory.getData("CAIR/voice_speed")) + "\\"
        except:
            self.memory.insertData("CAIR/voice_speed", 80)
            self.voice_speed = "\\RSPD=80\\"

        self.client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

        self.audio_device = ALProxy("ALAudioDevice")
        self.audio_device.setOutputVolume(40)

        self.asr_service = ALProxy("ALSpeechRecognition")
        self.asr_service.setAudioExpression(False)

    def get_sensor_data_thread(self):
        while self.isAlive:
            self.logger.info("Thread reading sensor data")
            self.people = self.memory.getData("SensorData/People")
            data = {"sentence": "", "dialogue_states": "", "sensor_data": {"people": self.people}}
            encoded_data = json.dumps(data)
            # compressed_data = zlib(encoded_data)
            response = requests.get(BASE, data=encoded_data, verify=False)
            self.logger.info("Server response:" + str(response.json()['reply']))
            time.sleep(1)

        self.logger.info("Exiting sensor data thread")
        self.behavior_manager.stopBehavior("sensordata/people_perception")

    def onInput_onStart(self):
        # Try connecting to the socket that records the audio
        self.logger.info("Trying to connect to the audio recorder socket")
        try:
            self.client_socket.connect((audio_recorder_host, audio_recorder_port))
        except:
            self.animated_speech.say(self.voice_speed + "I'm sorry, I can't connect to the socket. Try again later.",
                                     self.configuration)
            self.onInput_onStop()

        try:
            if self.al.getState() != "disabled":
                self.animated_speech.say(self.voice_speed + "Give me a moment. I need to disable Autonomous "
                                                            "Life before we start.", self.configuration)
                self.al.setState("disabled")
        except:
            pass

        self.motion.wakeUp()

        # With Pepper robot, preload the images to be set on the tablet during the conversation
        if self.tablet:
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/dialoguemanager/img/DialogueMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/dialoguemanager/img/ExecutionMode.png")
            self.tablet_service.preLoadImage("http://" + self.tablet_service.robotIp() +
                                             "/apps/dialoguemanager/img/PrivacyMode.png")
            self.tablet_service.showImage("http://" + self.tablet_service.robotIp() +
                                          "/apps/dialoguemanager/img/DialogueMode.png")

            # If it's the first time using the system, call the function that acquires the first state
            if not os.path.isfile(self.speakers_info_file_path):
                self.logger.info("First user!")
                # This function creates the speakers_info and the speakers_sequence_stats files and initializes them
                # with the info of a generic user
                welcome_sentence, _, _ = self.utils.acquire_initial_state("00000000-0000-0000-0000-000000000000", "User")
                self.logger.info(str(welcome_sentence))
                if "$" in welcome_sentence:
                    welcome_sentence = welcome_sentence.replace("$00000000-0000-0000-0000-000000000000", "User")
                self.animated_speech.say(self.voice_speed + "Welcome to CAIR!" + str(welcome_sentence), self.configuration)
                self.previous_sentence = "Welcome to CAIR!" + welcome_sentence
                # Store the reply to update previous reply
                reply = "Welcome to CAIR!" + welcome_sentence
            else:
                self.logger.info("Users are already present in the info file")
                self.animated_speech.say(self.voice_speed + "I missed you! What would you like to talk about?",
                                         self.configuration)
                self.previous_sentence = "I missed you! What would you like to talk about?"
                # Store the reply to update previous reply
                reply = "I missed you! What would you like to talk about?"

        # Start the behavior that senses people
        if self.behavior_manager.isBehaviorInstalled("sensordata/people_perception"):
            if not self.behavior_manager.isBehaviorRunning("sensordata/people_perception"):
                self.behavior_manager.runBehavior("sensordata/people_perception")

        sensor_data_thread = threading.Thread(target=self.get_sensor_data_thread)
        sensor_data_thread.start()

        # Retrieve the states of the users and save them in an array
        with open(self.speakers_info_file_path, 'r') as f:
            profiles_dict = json.load(f)

        profiles_dialogue_state = []
        for profile in profiles_dict:
            profiles_dialogue_state.append(profiles_dict[profile]["dialogue_state"])

        # Retrieve the json containing the matrices of the speakers stats and the mapping
        # "same_interaction", "successive_interaction", "same_interaction_prob", "successive_interaction_prob",
        # "mapping_index_speaker"
        with open(self.speakers_stats_file_path, 'r') as f:
            speakers_stats = json.load(f)

        prev_interaction_last_speaker = ""
        same_interaction_last_speaker = ""
        prev_speaker_topic = ""

        while self.isAlive:
            self.logger.info("** Listening **")
            # Tell the audio recorder that the client is ready to receive the user reply
            self.client_socket.send(b"ready")
            xml_string = self.client_socket.recv(256).decode('utf-8')

            if xml_string == "":
                self.animated_speech.say(self.voice_speed + "I'm sorry, something went wrong with the socket connection.",
                                         self.configuration)
                self.onInput_onStop()

            # Transform the xml string to an Element Tree
            tree = ET.ElementTree(ET.fromstring(xml_string))

            # Stats about the user talking have already been added during the registration, so here we should just
            # increment the elements of the matrices and recompute the probabilities
            # -> Loop through the tags in the xml sentence and update the matrices to keep track of who spoke after whom
            xml_profile_tags = tree.findall('profile_id')
            for i in range(len(xml_profile_tags)):
                profile_id = xml_profile_tags[i].attrib["value"]

                # Increase the number of times that the previous speaker said something (including unknown speaker)
                n_turns = profiles_dict[profile_id]["dialogue_state"]["n_turns"]
                profiles_dict[profile_id]["dialogue_state"]["n_turns"] = n_turns + 1

                # If it's the first speaker of the interaction (and it is not the first interaction),
                # check if the speaker is the same as the last one of the previous interaction
                if i == 0:
                    if prev_interaction_last_speaker != "":
                        # Increase the number of times that the previous speaker said something (including unknown speaker)
                        # n_turns = profiles_dict[prev_interaction_last_speaker]["dialogue_state"]["n_turns"]
                        # profiles_dict[prev_interaction_last_speaker]["dialogue_state"]["n_turns"] = n_turns + 1

                        # Increment the element of the "successive_interaction" matrix with the indexes mapped to
                        # row = prev_interaction_last_speaker, column = profile_id
                        row = speakers_stats["mapping_index_speaker"].index(prev_interaction_last_speaker)
                        column = speakers_stats["mapping_index_speaker"].index(profile_id)
                        speakers_stats["successive_interaction"][row][column] = \
                            speakers_stats["successive_interaction"][row][column] + 1
                        # Recompute the probabilities of the users talking after the previous one
                        for c in range(len(speakers_stats["mapping_index_speaker"])):
                            speakers_stats["successive_interaction_prob"][row][c] = \
                                speakers_stats["successive_interaction"][row][c] / \
                                profiles_dict[prev_interaction_last_speaker]["dialogue_state"]["n_turns"]
                else:
                    # Increase the number of times that the previous speaker said something (including unknown speaker)
                    # n_turns = profiles_dict[same_interaction_last_speaker]["dialogue_state"]["n_turns"]
                    # profiles_dict[same_interaction_last_speaker]["dialogue_state"]["n_turns"] = n_turns + 1

                    # Increment the element of the "same_interaction" matrix with th indexes mapped to
                    # row = same_interaction_last_speaker, column = profile_id
                    row = speakers_stats["mapping_index_speaker"].index(same_interaction_last_speaker)
                    column = speakers_stats["mapping_index_speaker"].index(profile_id)
                    speakers_stats["same_interaction"][row][column] = \
                        speakers_stats["same_interaction"][row][column] + 1
                    for c in range(len(speakers_stats["mapping_index_speaker"])):
                        speakers_stats["same_interaction_prob"][row][c] = \
                            speakers_stats["same_interaction"][row][c] / \
                            profiles_dict[same_interaction_last_speaker]["dialogue_state"]["n_turns"]

                same_interaction_last_speaker = profile_id

            # Compute the total number of turns of all users
            tot_turns = 0
            for profile_id in profiles_dict:
                tot_turns = tot_turns + profiles_dict[profile_id]["dialogue_state"]["n_turns"]

            # Recompute all the a priori probabilities with the updated number of turns
            for i in range(len(speakers_stats["mapping_index_speaker"])):
                # Get the profile id from the mapping array (so that they are in the same order of the probabilities)
                curr_id = speakers_stats["mapping_index_speaker"][i]
                # Get the number of turns of that user
                speaker_turns = profiles_dict[curr_id]["dialogue_state"]["n_turns"]
                # if curr_id == same_interaction_last_speaker:
                #    speaker_turns = speaker_turns + 1
                # Compute the a priori probability and update the value in the array
                speakers_stats["a_priori_prob"][i] = speaker_turns / tot_turns

            # Update content of the speaker stats file
            with open(self.speakers_stats_file_path, 'w') as cl_state:
                json.dump(speakers_stats, cl_state, ensure_ascii=False, indent=4)

            ###################################################
            # Parse the xml string and extract the first sentence and the first speaker
            # TODO: this is just to estimate who talked in the previous turn and to acquire exit/repeat sentences
            tree = ET.ElementTree(ET.fromstring(xml_string))
            sentence = tree.findall('profile_id')[0].text
            profile_id = tree.findall("profile_id")[0].attrib["value"]
            print(profiles_dict[profile_id]["name"] + ":", sentence)

            # Check if the user wants to exit or wants the robot to repeat the previous sentence
            sentence = sentence.replace(".", "")
            # Reset repeat to false, otherwise it will always repeat the previous sentence
            self.repeat = False

            # If the user said one of the "Exit Application keywords"
            if sentence.lower() in self.exit_keywords:
                self.isAlive = False
                self.animated_speech.say(self.voice_speed + "Ok, thank you for talking with me! Goodbye.",
                                         self.configuration)
                self.memory.insertData(self.speech_reco_event, [])
                self.onInput_onStop()
                sys.exit(0)
            # If the user said a Repeat keyword
            elif sentence.lower() in self.repeat_keywords:
                # If a previous sentence to repeat exists
                if self.previous_sentence:
                    self.repeat = True
                    self.animated_speech.say(self.voice_speed + "Sure. I said: " + str(self.previous_sentence),
                                             self.configuration)
                else:
                    self.animated_speech.say(self.voice_speed + "I'm sorry, I have nothing to repeat.",
                                             self.configuration)

            # If the user did not ask to exit or to repeat something, send the sentence to the server
            if not self.repeat:
                # Retrieve the states of the users and save them in an array
                profiles_dialogue_state = []
                for profile in profiles_dict:
                    profiles_dialogue_state.append(profiles_dict[profile]["dialogue_state"])

                # Compose the payload of the message to be sent to the server
                data = {"sentence": xml_string, "dialogue_states": profiles_dialogue_state,
                        "speakers_stats": speakers_stats,
                        "prev_speaker_info": {"id": prev_interaction_last_speaker, "topic": prev_speaker_topic},
                        "sensor_data": ""}

                encoded_data = json.dumps(data).encode('utf-8')
                compressed_data = zlib.compress(encoded_data)
                hub_response = requests.get(BASE, data=compressed_data, verify=False)
                # If the Hub cannot contact the dialogue service, the response will be empty
                if hub_response:
                    # Overwrite the array containing the states of the profiles with those contained in the Hub response
                    profiles_dialogue_state = hub_response.json()['dialogue_states']
                    speakers_stats = hub_response.json()["speakers_stats"]
                    # Update all the profile states stored locally on the client
                    for dialogue_state in profiles_dialogue_state:
                        profile_id = dialogue_state["profile_id"]
                        profiles_dict[profile_id]["dialogue_state"] = dialogue_state
                    # Store the updated dictionary in the file
                    with open(self.speakers_info_file_path, 'w') as f:
                        json.dump(profiles_dict, f, ensure_ascii=False, indent=4)
                    # The hub updates the average topic distance matrix, hence it should be written on the file
                    with open(self.speakers_stats_file_path, 'w') as f:
                        json.dump(speakers_stats, f, ensure_ascii=False, indent=4)
                else:
                    self.logger.info("No response received from the Hub!")
                    self.onInput_onStop()
                    exit(0)

                intent_reply = hub_response.json()['intent_reply']
                plan = hub_response.json()['plan']
                reply = hub_response.json()['reply']

                # Substitute the speaker name in place of the user id
                if "$" in reply:
                    for prof_id in profiles_dict:
                        if prof_id in reply:
                            reply = reply.replace("$" + prof_id, profiles_dict[prof_id]["name"])

                # If there is an intent reply, it means that something has been matched by the Plan manager service
                # Assume that when there is the intent reply there is also the plan
                if intent_reply:
                    self.logger.info(str(intent_reply))
                    self.logger.info(str(plan))
                    if "$" in intent_reply:
                        for prof_id in profiles_dict:
                            if prof_id in intent_reply:
                                intent_reply = intent_reply.replace("$" + prof_id, profiles_dict[prof_id]["name"])

                    # Say the intent reply and prepare the memory area in which is stored the starting sentence
                    # of a dialogue with a new user (in the eventuality that the plan contains the registration action)
                    self.animated_speech.say(self.voice_speed + str(intent_reply), self.configuration)
                    self.memory.insertData("CAIR/first_dialogue_sentence", "")

                    plan_items = plan.split("#")[1:]
                    self.logger.info(plan_items)
                    # For each action in the plan, check which action is it and execute it
                    # (if the corresponding behavior is installed)
                    for item in plan_items:
                        item = item.encode('utf-8')
                        self.plans.perform_action(item)

                    with open(self.speakers_info_file_path, 'r') as f:
                        profiles_dict = json.load(f)

                    with open(self.speakers_stats_file_path, 'r') as f:
                        speakers_stats = json.load(f)


                    # Once the execution of the plan is finished, set the dialogue mode image on the tablet
                    if self.tablet:
                        self.tablet_service.showImage("http://" + self.tablet_service.robotIp() +
                                                      "/apps/dialoguemanager/img/DialogueMode.png")

                    first_dialogue_sentence = self.memory.getData("CAIR/first_dialogue_sentence")
                    # If a new user has registered, there will be the initial sentence which should be said instead of
                    # the system reply
                    if first_dialogue_sentence:
                        reply = first_dialogue_sentence
                        # Empty the memory area
                        self.memory.insertData("CAIR/first_dialogue_sentence", "")

                # If there is a reply from the Cloud (or the first sentence), say it after the plan has been executed
                if reply:
                    self.logger.info(str(reply))
                    self.animated_speech.say(self.voice_speed + str(reply), self.configuration)
                    self.previous_sentence = reply

            # Update the info about id and topic of previous speaker to the current one, even if it asked to repeat
            prev_interaction_last_speaker = same_interaction_last_speaker
            prev_speaker_topic = profiles_dict[profile_id]["dialogue_state"]["topic"]

    def onInput_onStop(self):
        if self.tablet:
            self.tablet_service.hideImage()
        self.al.setState("interactive")
        self.utils.setAutonomousAbilities(True, True, True, True, True)
        # If present, delete the transformation - if the robot moves outside CAIR it is no more valid
        try:
            self.memory.removeData("CAIR/transformation_matrix")
            self.memory.removeData("CAIR/theta")
        except:
            self.logger.info("No transformation to delete in memory.")
            pass
        self.onStopped()
        self.onUnload()
        sys.exit(0)]]></content></script><Input name="onLoad" type="1" type_size="1" nature="0" inner="1" tooltip="Signal sent when diagram is loaded." id="1" /><Input name="onStart" type="1" type_size="1" nature="2" inner="0" tooltip="Box behavior starts when a signal is received on this input." id="2" /><Input name="onStop" type="1" type_size="1" nature="3" inner="0" tooltip="Box behavior stops when a signal is received on this input." id="3" /><Output name="onStopped" type="1" type_size="1" nature="1" inner="0" tooltip="Signal sent when box behavior is finished." id="4" /></Box><Link inputowner="1" indexofinput="2" outputowner="0" indexofoutput="2" /><Link inputowner="0" indexofinput="4" outputowner="1" indexofoutput="4" /></Diagram></BehaviorKeyframe></BehaviorLayer></Timeline></Box></ChoregrapheProject>